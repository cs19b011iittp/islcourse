# -*- coding: utf-8 -*-
"""cs19b011 isl endsem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KitJzuBXPcgp7EH_qc7KksyFUogq1k-6
"""

import torch
from torch import nn
import torch.optim as optim
from sklearn.datasets import fetch_openml
from sklearn.datasets import load_digits
import sklearn.cluster as skl_cluster
from sklearn.datasets import load_digits
from sklearn.datasets import make_blobs

from sklearn.datasets import make_circles
from sklearn.metrics import homogeneity_score
from sklearn.metrics import completeness_score
from sklearn.metrics.cluster import v_measure_score

from sklearn.datasets import make_blobs, make_circles
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_completeness_v_measure

def get_data_blobs(n_points=100):
  X, y = make_blobs(n_samples=n_points, centers=3, n_features=2,random_state=0)
  return X,y

def get_data_circles(n_points=100):
  X, y = make_circles(n_samples=n_points, random_state=0, factor=0.8)
  return X,y

def get_data_mnist():
  X, y = load_digits(return_X_y=True, as_frame=True)
  return X,y

def build_kmeans(X=None,k=10):
  km = skl_cluster.KMeans(n_clusters=k, random_state=0).fit(X)
  return km

def assign_kmeans(km=None,X=None):
  ypred = km.predict(X)
  return ypred

def compare_clusterings(ypred_1=None,ypred_2=None):
  h = homogeneity_score(ypred_1, ypred_2)
  c = completeness_score(ypred_1, ypred_2)
  v = v_measure_score(ypred_1, ypred_2)
  return h,c,v

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score
from sklearn.model_selection import GridSearchCV

def build_lr_model(X=None, y=None):
  lr_model = LogisticRegression()
  lr_model.fit(X, y)
  return lr_model

def build_rf_model(X=None, y=None):
  rf_model=RandomForestClassifier()
  rf_model.fit(X, y)
  return rf_model

def get_metrics(model=None,X=None,y=None):
  y_pred = model.predict(X)
  acc = accuracy_score(y, y_pred)
  prec = precision_score(y, y_pred)
  rec = recall_score(y, y_pred)
  f1 = f1_score(y, y_pred)
  auc = roc_auc_score(y, y_pred)
  return acc, prec, rec, f1, auc

def get_paramgrid_lr():
  lr_param_grid = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}
  return lr_param_grid

def get_paramgrid_rf():
  rf_param_grid = {'bootstrap': [True, False],
                  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
                  'max_features': ['auto', 'sqrt'],
                  'min_samples_leaf': [1, 2, 4],
                  'min_samples_split': [2, 5, 10],
                  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
  return rf_param_grid

def perform_gridsearch_cv_multimetric(model=None, param_grid=None, cv=5, X=None, y=None, metrics=['accuracy','roc_auc']):
  
  top1_scores = []
  for score in metrics:
     grid_search_cv = GridSearchCV(model, param_grid, scoring=score)
     grid_search_cv.fit(X, y)
     top1_scores(grid_search_cv.best_score_)
  
  return top1_scores


class MyNN(nn.Module):
  def __init__(self,inp_dim=64,hid_dim=13,num_classes=10):
    super(MyNN,self)
    
    self.fc_encoder = None # write your code inp_dim to hid_dim mapper
    self.fc_decoder = None # write your code hid_dim to inp_dim mapper
    self.fc_classifier = None # write your code to map hid_dim to num_classes
    
    self.relu = None #write your code - relu object
    self.softmax = None #write your code - softmax object
    
  def forward(self,x):
    x = None # write your code - flatten x
    x_enc = self.fc_encoder(x)
    x_enc = self.relu(x_enc)
    
    y_pred = self.fc_classifier(x_enc)
    y_pred = self.softmax(y_pred)
    
    x_dec = self.fc_decoder(x_enc)
    
    return y_pred, x_dec
  
  # This a multi component loss function - lc1 for class prediction loss and lc2 for auto-encoding loss
  def loss_fn(self,x,yground,y_pred,xencdec):
    
    # class prediction loss
    # yground needs to be one hot encoded - write your code
    lc1 = None # write your code for cross entropy between yground and y_pred, advised to use torch.mean()
    
    # auto encoding loss
    lc2 = torch.mean((x - xencdec)**2)
    
    lval = lc1 + lc2
    
    return lval
    
def get_mynn(inp_dim=64,hid_dim=13,num_classes=10):
  mynn = MyNN(inp_dim,hid_dim,num_classes)
  mynn.double()
  return mynn

def get_mnist_tensor():
  # download sklearn mnist
  # convert to tensor
  X, y = None, None
  # write your code
  return X,y

def get_loss_on_single_point(mynn=None,x0,y0):
  y_pred, xencdec = mynn(x0)
  lossval = mynn.loss_fn(x0,y0,y_pred,xencdec)
  # the lossval should have grad_fn attribute set
  return lossval

def train_combined_encdec_predictor(mynn=None,X,y, epochs=11):
  # X, y are provided as tensor
  # perform training on the entire data set (no batches etc.)
  # for each epoch, update weights
  
  optimizer = optim.SGD(mynn.parameters(), lr=0.01)
  
  for i in range(epochs):
    optimizer.zero_grad()
    ypred, Xencdec = mynn(X)
    lval = mynn.loss_fn(X,y,ypred,Xencdec)
    lval.backward()
    optimzer.step()
    
  return mynn
